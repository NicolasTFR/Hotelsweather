{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_destination=[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkign where we are, we should be in kayakproject\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\main'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#going to main folder\n",
    "os.chdir(\"main\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 12:26:32 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:32 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:32 [scrapy.extensions.telnet] INFO: Telnet Password: 53d733fa665f60ca\n",
      "2022-05-03 12:26:32 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:32 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:32 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:26:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:26:35 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Mont Saint Michel_main.json\n",
      "2022-05-03 12:26:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2105,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310975,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.062621,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 26, 35, 826906),\n",
      " 'httpcompression/response_bytes': 1957240,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 32, 764285)}\n",
      "2022-05-03 12:26:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:26:37 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:37 [scrapy.extensions.telnet] INFO: Telnet Password: 5ea8736f40d31a58\n",
      "2022-05-03 12:26:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:26:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:26:40 [scrapy.extensions.feedexport] INFO: Stored json feed (52 items) in: results\\St Malo_main.json\n",
      "2022-05-03 12:26:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2083,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306670,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.166184,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 26, 40, 933538),\n",
      " 'httpcompression/response_bytes': 1987292,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 52,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 37, 767354)}\n",
      "2022-05-03 12:26:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:26:42 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:42 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:42 [scrapy.extensions.telnet] INFO: Telnet Password: 2207112aa611adde\n",
      "2022-05-03 12:26:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:26:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:26:46 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Bayeux_main.json\n",
      "2022-05-03 12:26:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2094,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305402,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.17382,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 26, 46, 18881),\n",
      " 'httpcompression/response_bytes': 1914258,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 42, 845061)}\n",
      "2022-05-03 12:26:46 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:26:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:47 [scrapy.extensions.telnet] INFO: Telnet Password: 8139a3c5d1e7e0dc\n",
      "2022-05-03 12:26:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:26:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:26:51 [scrapy.extensions.feedexport] INFO: Stored json feed (63 items) in: results\\Le Havre_main.json\n",
      "2022-05-03 12:26:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2082,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311387,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.162168,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 26, 51, 96550),\n",
      " 'httpcompression/response_bytes': 1988038,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 63,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 47, 934382)}\n",
      "2022-05-03 12:26:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:26:52 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:52 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:52 [scrapy.extensions.telnet] INFO: Telnet Password: 68f06994319dbdec\n",
      "2022-05-03 12:26:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:26:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:26:55 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Rouen_main.json\n",
      "2022-05-03 12:26:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2063,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310059,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.752407,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 26, 55, 689787),\n",
      " 'httpcompression/response_bytes': 1932290,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 52, 937380)}\n",
      "2022-05-03 12:26:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:26:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:26:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:26:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:26:57 [scrapy.extensions.telnet] INFO: Telnet Password: 2fb4148284fac4db\n",
      "2022-05-03 12:26:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:26:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:26:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:26:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:26:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:26:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:26:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:00 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Paris_main.json\n",
      "2022-05-03 12:27:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2081,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311079,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.14577,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 0, 738847),\n",
      " 'httpcompression/response_bytes': 2012732,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 26, 57, 593077)}\n",
      "2022-05-03 12:27:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:02 [scrapy.extensions.telnet] INFO: Telnet Password: a4ba681a613aa9c9\n",
      "2022-05-03 12:27:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:05 [scrapy.extensions.feedexport] INFO: Stored json feed (42 items) in: results\\Amiens_main.json\n",
      "2022-05-03 12:27:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2082,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310464,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.087349,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 5, 696603),\n",
      " 'httpcompression/response_bytes': 1932935,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 2, 609254)}\n",
      "2022-05-03 12:27:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:07 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:07 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:07 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:07 [scrapy.extensions.telnet] INFO: Telnet Password: 5541083702e3eba2\n",
      "2022-05-03 12:27:07 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:10 [scrapy.extensions.feedexport] INFO: Stored json feed (44 items) in: results\\Lille_main.json\n",
      "2022-05-03 12:27:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2079,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311885,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.191279,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 10, 709792),\n",
      " 'httpcompression/response_bytes': 2005096,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 44,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 7, 518513)}\n",
      "2022-05-03 12:27:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:12 [scrapy.extensions.telnet] INFO: Telnet Password: f5b0c6f55e1951fb\n",
      "2022-05-03 12:27:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:15 [scrapy.extensions.feedexport] INFO: Stored json feed (42 items) in: results\\Strasbourg_main.json\n",
      "2022-05-03 12:27:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2098,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305426,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.211615,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 15, 772279),\n",
      " 'httpcompression/response_bytes': 1974881,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 12, 560664)}\n",
      "2022-05-03 12:27:15 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:17 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:17 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:17 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:17 [scrapy.extensions.telnet] INFO: Telnet Password: ad6c08b05e2d916a\n",
      "2022-05-03 12:27:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:17 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:20 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Chateau du Haut Koenigsbourg_main.json\n",
      "2022-05-03 12:27:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2146,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311191,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.755059,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 20, 566872),\n",
      " 'httpcompression/response_bytes': 1970057,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 17, 811813)}\n",
      "2022-05-03 12:27:20 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:21 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:21 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:21 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:21 [scrapy.extensions.telnet] INFO: Telnet Password: e73a08f5c28c872f\n",
      "2022-05-03 12:27:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:25 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: results\\Colmar_main.json\n",
      "2022-05-03 12:27:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311212,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.144685,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 25, 437152),\n",
      " 'httpcompression/response_bytes': 1979592,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 38,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 22, 292467)}\n",
      "2022-05-03 12:27:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:26 [scrapy.extensions.telnet] INFO: Telnet Password: 53c471307fa9f82a\n",
      "2022-05-03 12:27:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:27 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:27 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:27 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:27 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:30 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Eguisheim_main.json\n",
      "2022-05-03 12:27:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2095,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307539,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.241445,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 30, 453588),\n",
      " 'httpcompression/response_bytes': 1965100,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 27, 212143)}\n",
      "2022-05-03 12:27:30 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:31 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:31 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:31 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:31 [scrapy.extensions.telnet] INFO: Telnet Password: 8eddf33b61f5311e\n",
      "2022-05-03 12:27:31 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:31 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:31 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:35 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Besancon_main.json\n",
      "2022-05-03 12:27:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2094,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306358,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.148528,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 35, 242766),\n",
      " 'httpcompression/response_bytes': 1924438,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 32, 94238)}\n",
      "2022-05-03 12:27:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:36 [scrapy.extensions.telnet] INFO: Telnet Password: ee1c906420fab2b0\n",
      "2022-05-03 12:27:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:40 [scrapy.extensions.feedexport] INFO: Stored json feed (44 items) in: results\\Dijon_main.json\n",
      "2022-05-03 12:27:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2077,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306114,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.534829,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 40, 535077),\n",
      " 'httpcompression/response_bytes': 1951320,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 44,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 37, 248)}\n",
      "2022-05-03 12:27:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:41 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:41 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:41 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:41 [scrapy.extensions.telnet] INFO: Telnet Password: aed4d766785484f5\n",
      "2022-05-03 12:27:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:41 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:41 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:41 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:45 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Annecy_main.json\n",
      "2022-05-03 12:27:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2088,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306802,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.261504,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 45, 421511),\n",
      " 'httpcompression/response_bytes': 1962283,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 42, 160007)}\n",
      "2022-05-03 12:27:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:46 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:46 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:46 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:46 [scrapy.extensions.telnet] INFO: Telnet Password: f9dd153afe8e20f4\n",
      "2022-05-03 12:27:46 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:50 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Grenoble_main.json\n",
      "2022-05-03 12:27:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2092,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306675,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.137375,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 50, 493761),\n",
      " 'httpcompression/response_bytes': 1941657,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 47, 356386)}\n",
      "2022-05-03 12:27:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:51 [scrapy.extensions.telnet] INFO: Telnet Password: 1e67c35b60c0c814\n",
      "2022-05-03 12:27:51 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:51 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:51 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:51 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:51 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:27:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:27:55 [scrapy.extensions.feedexport] INFO: Stored json feed (44 items) in: results\\Lyon_main.json\n",
      "2022-05-03 12:27:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2054,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309876,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.343912,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 27, 55, 504690),\n",
      " 'httpcompression/response_bytes': 1981649,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 44,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 52, 160778)}\n",
      "2022-05-03 12:27:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:27:56 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:27:56 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:27:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:27:56 [scrapy.extensions.telnet] INFO: Telnet Password: af37e8f9cdf20b0f\n",
      "2022-05-03 12:27:56 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:27:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:27:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:27:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:27:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:27:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:27:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:00 [scrapy.extensions.feedexport] INFO: Stored json feed (39 items) in: results\\Gorges du Verdon_main.json\n",
      "2022-05-03 12:28:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2116,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 308723,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.925736,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 0, 296549),\n",
      " 'httpcompression/response_bytes': 1924186,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 39,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 27, 57, 370813)}\n",
      "2022-05-03 12:28:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:01 [scrapy.extensions.telnet] INFO: Telnet Password: ece166e192b362ef\n",
      "2022-05-03 12:28:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:05 [scrapy.extensions.feedexport] INFO: Stored json feed (58 items) in: results\\Bormes les Mimosas_main.json\n",
      "2022-05-03 12:28:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2114,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 312274,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.111553,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 5, 163911),\n",
      " 'httpcompression/response_bytes': 2027834,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 58,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 2, 52358)}\n",
      "2022-05-03 12:28:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:06 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:06 [scrapy.extensions.telnet] INFO: Telnet Password: b6f4ff849b9e7226\n",
      "2022-05-03 12:28:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:10 [scrapy.extensions.feedexport] INFO: Stored json feed (58 items) in: results\\Cassis_main.json\n",
      "2022-05-03 12:28:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2074,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 313268,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.361777,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 10, 393972),\n",
      " 'httpcompression/response_bytes': 2037634,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 58,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 7, 32195)}\n",
      "2022-05-03 12:28:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:11 [scrapy.extensions.telnet] INFO: Telnet Password: 0471d753056ad2a1\n",
      "2022-05-03 12:28:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:15 [scrapy.extensions.feedexport] INFO: Stored json feed (52 items) in: results\\Marseille_main.json\n",
      "2022-05-03 12:28:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2097,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 308751,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.098155,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 15, 297497),\n",
      " 'httpcompression/response_bytes': 2025335,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 52,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 12, 199342)}\n",
      "2022-05-03 12:28:15 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:16 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:16 [scrapy.extensions.telnet] INFO: Telnet Password: 4ade9eac2c8b7c53\n",
      "2022-05-03 12:28:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:16 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:16 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:16 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:16 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:20 [scrapy.extensions.feedexport] INFO: Stored json feed (39 items) in: results\\Aix en Provence_main.json\n",
      "2022-05-03 12:28:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2111,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306331,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.180327,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 20, 340714),\n",
      " 'httpcompression/response_bytes': 1955923,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 39,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 17, 160387)}\n",
      "2022-05-03 12:28:20 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:21 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:21 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:21 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:21 [scrapy.extensions.telnet] INFO: Telnet Password: c82e960652805740\n",
      "2022-05-03 12:28:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:21 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:25 [scrapy.extensions.feedexport] INFO: Stored json feed (40 items) in: results\\Avignon_main.json\n",
      "2022-05-03 12:28:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2085,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 308435,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.428542,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 25, 510728),\n",
      " 'httpcompression/response_bytes': 1996273,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 40,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 22, 82186)}\n",
      "2022-05-03 12:28:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:26 [scrapy.extensions.telnet] INFO: Telnet Password: ce7846499e2ef7a0\n",
      "2022-05-03 12:28:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:26 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:26 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:30 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: results\\Uzes_main.json\n",
      "2022-05-03 12:28:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2084,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306355,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.535289,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 30, 724707),\n",
      " 'httpcompression/response_bytes': 1907098,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 38,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 27, 189418)}\n",
      "2022-05-03 12:28:30 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:32 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:32 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:32 [scrapy.extensions.telnet] INFO: Telnet Password: d03c24b352ec3f18\n",
      "2022-05-03 12:28:32 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:32 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:32 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:32 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:32 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:35 [scrapy.extensions.feedexport] INFO: Stored json feed (40 items) in: results\\Nimes_main.json\n",
      "2022-05-03 12:28:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2087,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309652,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.155391,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 35, 808170),\n",
      " 'httpcompression/response_bytes': 1983837,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 40,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 32, 652779)}\n",
      "2022-05-03 12:28:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:37 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:37 [scrapy.extensions.telnet] INFO: Telnet Password: 8c9674b27fb486d1\n",
      "2022-05-03 12:28:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:41 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:41 [scrapy.extensions.feedexport] INFO: Stored json feed (40 items) in: results\\Aigues Mortes_main.json\n",
      "2022-05-03 12:28:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2089,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307423,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.080586,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 41, 7650),\n",
      " 'httpcompression/response_bytes': 1968481,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 40,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 37, 927064)}\n",
      "2022-05-03 12:28:41 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:42 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:42 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:42 [scrapy.extensions.telnet] INFO: Telnet Password: 1424052bac118668\n",
      "2022-05-03 12:28:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:45 [scrapy.extensions.feedexport] INFO: Stored json feed (59 items) in: results\\Saintes Maries de la mer_main.json\n",
      "2022-05-03 12:28:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2140,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 304012,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.317347,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 45, 992344),\n",
      " 'httpcompression/response_bytes': 1963478,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 59,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 42, 674997)}\n",
      "2022-05-03 12:28:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:47 [scrapy.extensions.telnet] INFO: Telnet Password: 6d313b7a35fa5735\n",
      "2022-05-03 12:28:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:50 [scrapy.extensions.feedexport] INFO: Stored json feed (63 items) in: results\\Collioure_main.json\n",
      "2022-05-03 12:28:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2099,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311275,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.11502,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 50, 885565),\n",
      " 'httpcompression/response_bytes': 2030721,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 63,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 47, 770545)}\n",
      "2022-05-03 12:28:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:52 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:52 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:52 [scrapy.extensions.telnet] INFO: Telnet Password: bb7c3787422d04d6\n",
      "2022-05-03 12:28:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:28:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:28:55 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Carcassonne_main.json\n",
      "2022-05-03 12:28:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2111,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306561,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.097572,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 28, 55, 759258),\n",
      " 'httpcompression/response_bytes': 1978980,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 52, 661686)}\n",
      "2022-05-03 12:28:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:28:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:28:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:28:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:28:57 [scrapy.extensions.telnet] INFO: Telnet Password: c321c3d2cbaf8ab9\n",
      "2022-05-03 12:28:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:28:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:28:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:28:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:28:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:28:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:28:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:00 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: results\\Ariege_main.json\n",
      "2022-05-03 12:29:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2078,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305638,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.123385,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 0, 669596),\n",
      " 'httpcompression/response_bytes': 1938511,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 36,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 28, 57, 546211)}\n",
      "2022-05-03 12:29:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:29:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:29:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:29:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:29:01 [scrapy.extensions.telnet] INFO: Telnet Password: 0df91e1103cda3a0\n",
      "2022-05-03 12:29:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:29:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:29:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:29:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:29:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:29:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:29:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:05 [scrapy.extensions.feedexport] INFO: Stored json feed (39 items) in: results\\Toulouse_main.json\n",
      "2022-05-03 12:29:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306687,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.128537,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 5, 551365),\n",
      " 'httpcompression/response_bytes': 1984623,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 39,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 29, 2, 422828)}\n",
      "2022-05-03 12:29:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:29:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:29:06 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:29:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:29:06 [scrapy.extensions.telnet] INFO: Telnet Password: f8a4f09b3da8a726\n",
      "2022-05-03 12:29:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:29:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:29:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:29:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:29:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:29:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:29:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:10 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: results\\Montauban_main.json\n",
      "2022-05-03 12:29:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2101,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310567,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.478357,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 10, 809261),\n",
      " 'httpcompression/response_bytes': 1932130,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 36,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 29, 7, 330904)}\n",
      "2022-05-03 12:29:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:29:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:29:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:29:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:29:12 [scrapy.extensions.telnet] INFO: Telnet Password: c573fa1d6ef4e473\n",
      "2022-05-03 12:29:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:29:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:29:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:29:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:29:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:29:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:29:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:16 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:16 [scrapy.extensions.feedexport] INFO: Stored json feed (60 items) in: results\\Biarritz_main.json\n",
      "2022-05-03 12:29:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2100,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307392,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.582199,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 16, 169184),\n",
      " 'httpcompression/response_bytes': 2009757,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 60,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 29, 12, 586985)}\n",
      "2022-05-03 12:29:16 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:29:17 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:29:17 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:29:17 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:29:17 [scrapy.extensions.telnet] INFO: Telnet Password: ea5d38aeed582671\n",
      "2022-05-03 12:29:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:29:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:29:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:29:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:29:17 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:29:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:29:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:21 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:21 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Bayonne_main.json\n",
      "2022-05-03 12:29:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2077,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 304416,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.298201,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 21, 285531),\n",
      " 'httpcompression/response_bytes': 1893424,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 29, 17, 987330)}\n",
      "2022-05-03 12:29:21 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-03 12:29:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-03 12:29:22 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-03 12:29:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-03 12:29:22 [scrapy.extensions.telnet] INFO: Telnet Password: 57cbb39489fc031d\n",
      "2022-05-03 12:29:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-03 12:29:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-03 12:29:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-03 12:29:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-03 12:29:23 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-03 12:29:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-03 12:29:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-03 12:29:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-03 12:29:27 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\La Rochelle_main.json\n",
      "2022-05-03 12:29:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2107,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310684,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.581316,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 3, 10, 29, 27, 93528),\n",
      " 'httpcompression/response_bytes': 1956788,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 3, 10, 29, 23, 512212)}\n",
      "2022-05-03 12:29:27 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n"
     ]
    }
   ],
   "source": [
    "#scraping the search results page from booking for the destinations. Spider used is in bookingmain.py, autothrottle is on and ROBOTSTXT_OBEY is False to avoid issues with booking.com\n",
    "for destination in list_destination:\n",
    "    os.environ[\"listdestination\"]=destination\n",
    "    !scrapy crawl bookingmain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we scraped empty lines as the structure of each pages varies slighlty, so defining a function to clean the data and remove the nulls\n",
    "def clean_nones(value):\n",
    "    \"\"\"\n",
    "    Recursively remove all None values from dictionaries and lists, and returns\n",
    "    the result as a new dictionary or list.\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return [clean_nones(x) for x in value if x is not None]\n",
    "    elif isinstance(value, dict):\n",
    "        return {\n",
    "            key: clean_nones(val)\n",
    "            for key, val in value.items()\n",
    "            if val is not None\n",
    "        }\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we go to the results folder where the scrapped output files are\n",
    "os.chdir(\"results\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for everyfiles, opening then, cleaning them, and saving them\n",
    "for files in os.listdir():\n",
    "    with open(files) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        datacleaned=clean_nones(data)\n",
    "        datacleaned2=list(filter(None, datacleaned))\n",
    "        with open(files, 'w') as f:\n",
    "            json.dump(datacleaned2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\main'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we go to the detailed folder, as scrapy does not allow 2 different spiders to be launched in the same folder\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"detailed\")\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 16:30:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:30:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:30:45 [scrapy.extensions.telnet] INFO: Telnet Password: 6c5471312aaad27d\n",
      "2022-05-02 16:30:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:30:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:30:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:30:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:30:56 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:30:56 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Aigues Mortes_detailed.json\n",
      "2022-05-02 16:30:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24930,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5622324,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.024687,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 30, 56, 310470),\n",
      " 'httpcompression/response_bytes': 32819535,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 30, 46, 285783)}\n",
      "2022-05-02 16:30:56 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:30:56 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:30:56 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:30:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:30:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:30:57 [scrapy.extensions.telnet] INFO: Telnet Password: 6756204860aafdd7\n",
      "2022-05-02 16:30:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:30:58 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:30:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:30:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:09 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Aix en Provence_detailed.json\n",
      "2022-05-02 16:31:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24911,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5843812,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.142214,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 9, 472531),\n",
      " 'httpcompression/response_bytes': 35542418,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 30, 58, 330317)}\n",
      "2022-05-02 16:31:09 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:09 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:10 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:10 [scrapy.extensions.telnet] INFO: Telnet Password: 877eb4dbeec4073b\n",
      "2022-05-02 16:31:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:20 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Amiens_detailed.json\n",
      "2022-05-02 16:31:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24982,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5684188,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.609137,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 20, 953028),\n",
      " 'httpcompression/response_bytes': 33519655,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 11, 343891)}\n",
      "2022-05-02 16:31:20 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:22 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:22 [scrapy.extensions.telnet] INFO: Telnet Password: 6873d1941966a4eb\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:33 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Annecy_detailed.json\n",
      "2022-05-02 16:31:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24991,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5823304,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.8725,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 33, 689023),\n",
      " 'httpcompression/response_bytes': 35556367,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 22, 816523)}\n",
      "2022-05-02 16:31:33 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:35 [scrapy.extensions.telnet] INFO: Telnet Password: 91e76f4676bb53ff\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:45 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Ariege_detailed.json\n",
      "2022-05-02 16:31:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24930,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5678429,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.283151,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 45, 914972),\n",
      " 'httpcompression/response_bytes': 33385939,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 35, 631821)}\n",
      "2022-05-02 16:31:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:47 [scrapy.extensions.telnet] INFO: Telnet Password: e70a3e95cb2d0d88\n",
      "2022-05-02 16:31:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:48 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:59 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Avignon_detailed.json\n",
      "2022-05-02 16:31:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24965,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5926076,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.006819,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 59, 337442),\n",
      " 'httpcompression/response_bytes': 38008892,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 48, 330623)}\n",
      "2022-05-02 16:31:59 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:59 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:59 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:59 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:00 [scrapy.extensions.telnet] INFO: Telnet Password: f23c7d8285b44c9c\n",
      "2022-05-02 16:32:00 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:11 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:11 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bayeux_detailed.json\n",
      "2022-05-02 16:32:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24805,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5808008,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.060355,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 11, 269894),\n",
      " 'httpcompression/response_bytes': 35552099,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 1, 209539)}\n",
      "2022-05-02 16:32:11 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:12 [scrapy.extensions.telnet] INFO: Telnet Password: 848dba20b5520c42\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:23 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bayonne_detailed.json\n",
      "2022-05-02 16:32:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24867,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5707761,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.983078,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 23, 87524),\n",
      " 'httpcompression/response_bytes': 34140267,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 13, 104446)}\n",
      "2022-05-02 16:32:23 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:23 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:23 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:24 [scrapy.extensions.telnet] INFO: Telnet Password: b927994f8b70336a\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:24 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:35 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Besancon_detailed.json\n",
      "2022-05-02 16:32:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24884,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5811178,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.745808,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 35, 723073),\n",
      " 'httpcompression/response_bytes': 34551382,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 24, 977265)}\n",
      "2022-05-02 16:32:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:36 [scrapy.extensions.telnet] INFO: Telnet Password: 7d409fb34c7e0593\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:48 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Biarritz_detailed.json\n",
      "2022-05-02 16:32:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25134,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6314311,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.614111,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 48, 51794),\n",
      " 'httpcompression/response_bytes': 38777450,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 37, 437683)}\n",
      "2022-05-02 16:32:48 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:48 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:48 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:48 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:49 [scrapy.extensions.telnet] INFO: Telnet Password: 4f50be37eee9c97b\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:00 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bormes les Mimosas_detailed.json\n",
      "2022-05-02 16:33:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25175,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6165989,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.31939,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 0, 44808),\n",
      " 'httpcompression/response_bytes': 38179882,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 49, 725418)}\n",
      "2022-05-02 16:33:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:01 [scrapy.extensions.telnet] INFO: Telnet Password: d80d90a5f6463caf\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:13 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Carcassonne_detailed.json\n",
      "2022-05-02 16:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27393,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6315637,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 11.197834,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 13, 47847),\n",
      " 'httpcompression/response_bytes': 39878341,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 1, 850013)}\n",
      "2022-05-02 16:33:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:13 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:14 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:14 [scrapy.extensions.telnet] INFO: Telnet Password: b60b461441027289\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:14 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:25 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Cassis_detailed.json\n",
      "2022-05-02 16:33:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25506,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6097021,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.626818,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 25, 352824),\n",
      " 'httpcompression/response_bytes': 35430816,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 14, 726006)}\n",
      "2022-05-02 16:33:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:25 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:26 [scrapy.extensions.telnet] INFO: Telnet Password: 5ad6ed4da002b27c\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:38 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Chateau du Haut Koenigsbourg_detailed.json\n",
      "2022-05-02 16:33:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24818,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5865589,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.024899,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 38, 53632),\n",
      " 'httpcompression/response_bytes': 35961630,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 27, 28733)}\n",
      "2022-05-02 16:33:38 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:39 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:39 [scrapy.extensions.telnet] INFO: Telnet Password: 55cbdf8ec2f1ce76\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:50 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Collioure_detailed.json\n",
      "2022-05-02 16:33:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25311,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6092438,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.823191,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 50, 586849),\n",
      " 'httpcompression/response_bytes': 36878528,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 39, 763658)}\n",
      "2022-05-02 16:33:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:50 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:51 [scrapy.extensions.telnet] INFO: Telnet Password: 0bad3304765272c8\n",
      "2022-05-02 16:33:51 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:03 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Colmar_detailed.json\n",
      "2022-05-02 16:34:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24929,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5806109,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.029488,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 3, 306885),\n",
      " 'httpcompression/response_bytes': 35034620,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 52, 277397)}\n",
      "2022-05-02 16:34:03 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:03 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:04 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:04 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:04 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:04 [scrapy.extensions.telnet] INFO: Telnet Password: 26da2104340f86c6\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:16 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:16 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Dijon_detailed.json\n",
      "2022-05-02 16:34:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24853,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5913873,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.261334,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 16, 430800),\n",
      " 'httpcompression/response_bytes': 37418300,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 5, 169466)}\n",
      "2022-05-02 16:34:16 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:16 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:17 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:17 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:17 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:17 [scrapy.extensions.telnet] INFO: Telnet Password: c283dcb80c67a5a9\n",
      "2022-05-02 16:34:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:18 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:28 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:28 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Eguisheim_detailed.json\n",
      "2022-05-02 16:34:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24990,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5754106,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.395015,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 28, 686624),\n",
      " 'httpcompression/response_bytes': 33955889,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 18, 291609)}\n",
      "2022-05-02 16:34:28 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:28 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:28 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:29 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:30 [scrapy.extensions.telnet] INFO: Telnet Password: 48a66e1bf6df04c8\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:30 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:40 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Gorges du Verdon_detailed.json\n",
      "2022-05-02 16:34:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24928,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5723009,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.35252,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 40, 774978),\n",
      " 'httpcompression/response_bytes': 35249950,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 30, 422458)}\n",
      "2022-05-02 16:34:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:42 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:42 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:42 [scrapy.extensions.telnet] INFO: Telnet Password: 8af9aeaf38793693\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:53 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Grenoble_detailed.json\n",
      "2022-05-02 16:34:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24906,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5860228,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.02558,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 53, 139675),\n",
      " 'httpcompression/response_bytes': 34823553,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 43, 114095)}\n",
      "2022-05-02 16:34:53 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:53 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:54 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:54 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:54 [scrapy.extensions.telnet] INFO: Telnet Password: 836f0d30c097bd56\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:54 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:05 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/La Rochelle_detailed.json\n",
      "2022-05-02 16:35:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25039,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5855300,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.708969,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 5, 624440),\n",
      " 'httpcompression/response_bytes': 35996640,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 54, 915471)}\n",
      "2022-05-02 16:35:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:05 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:05 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:06 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:06 [scrapy.extensions.telnet] INFO: Telnet Password: 0798ce6251fe9a4b\n",
      "2022-05-02 16:35:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:19 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:19 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Le Havre_detailed.json\n",
      "2022-05-02 16:35:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25396,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6224853,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.854452,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 19, 128756),\n",
      " 'httpcompression/response_bytes': 36973944,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 7, 274304)}\n",
      "2022-05-02 16:35:19 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:20 [scrapy.extensions.telnet] INFO: Telnet Password: b26915a288ce79f5\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:32 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Lille_detailed.json\n",
      "2022-05-02 16:35:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25072,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5911274,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.311306,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 32, 86134),\n",
      " 'httpcompression/response_bytes': 36294029,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 20, 774828)}\n",
      "2022-05-02 16:35:32 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:32 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:32 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:33 [scrapy.extensions.telnet] INFO: Telnet Password: 276de36171cca6cf\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:33 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:44 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Lyon_detailed.json\n",
      "2022-05-02 16:35:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24947,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5797289,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.155747,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 44, 340926),\n",
      " 'httpcompression/response_bytes': 34648116,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 34, 185179)}\n",
      "2022-05-02 16:35:44 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:44 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:44 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:45 [scrapy.extensions.telnet] INFO: Telnet Password: 318d2fac28f08e9a\n",
      "2022-05-02 16:35:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:57 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:57 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Marseille_detailed.json\n",
      "2022-05-02 16:35:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27666,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6223310,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 11.259051,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 57, 493127),\n",
      " 'httpcompression/response_bytes': 37318376,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 46, 234076)}\n",
      "2022-05-02 16:35:57 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:58 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:58 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:58 [scrapy.extensions.telnet] INFO: Telnet Password: c140ab543725b78c\n",
      "2022-05-02 16:35:58 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:11 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:11 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Mont Saint Michel_detailed.json\n",
      "2022-05-02 16:36:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 26066,\n",
      " 'downloader/request_count': 26,\n",
      " 'downloader/request_method_count/GET': 26,\n",
      " 'downloader/response_bytes': 6065378,\n",
      " 'downloader/response_count': 26,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'downloader/response_status_count/500': 1,\n",
      " 'elapsed_time_seconds': 12.10855,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 11, 470431),\n",
      " 'httpcompression/response_bytes': 32715322,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'retry/count': 1,\n",
      " 'retry/reason_count/500 Internal Server Error': 1,\n",
      " 'scheduler/dequeued': 26,\n",
      " 'scheduler/dequeued/memory': 26,\n",
      " 'scheduler/enqueued': 26,\n",
      " 'scheduler/enqueued/memory': 26,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 59, 361881)}\n",
      "2022-05-02 16:36:11 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:12 [scrapy.extensions.telnet] INFO: Telnet Password: f16ab49b2f1c82c5\n",
      "2022-05-02 16:36:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:23 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Montauban_detailed.json\n",
      "2022-05-02 16:36:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24858,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5652581,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.678347,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 23, 977527),\n",
      " 'httpcompression/response_bytes': 32822271,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 13, 299180)}\n",
      "2022-05-02 16:36:23 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:25 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:25 [scrapy.extensions.telnet] INFO: Telnet Password: 3d4803bf746ca69d\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:36 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Nimes_detailed.json\n",
      "2022-05-02 16:36:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27578,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6222412,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 10.941203,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 36, 739491),\n",
      " 'httpcompression/response_bytes': 37343266,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 25, 798288)}\n",
      "2022-05-02 16:36:36 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:38 [scrapy.extensions.telnet] INFO: Telnet Password: bfc91e13a159c2bb\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:38 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:49 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Paris_detailed.json\n",
      "2022-05-02 16:36:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24709,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5771755,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.334221,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 49, 57433),\n",
      " 'httpcompression/response_bytes': 34592765,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 38, 723212)}\n",
      "2022-05-02 16:36:49 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:50 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:50 [scrapy.extensions.telnet] INFO: Telnet Password: ee03434bc74424ab\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:50 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:01 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Rouen_detailed.json\n",
      "2022-05-02 16:37:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24990,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5734031,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.392263,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 1, 360796),\n",
      " 'httpcompression/response_bytes': 34223928,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 50, 968533)}\n",
      "2022-05-02 16:37:01 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:02 [scrapy.extensions.telnet] INFO: Telnet Password: 13d4c2331262245d\n",
      "2022-05-02 16:37:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:03 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:13 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Saintes Maries de la mer_detailed.json\n",
      "2022-05-02 16:37:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25275,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6160184,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.460193,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 13, 686030),\n",
      " 'httpcompression/response_bytes': 38407198,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 3, 225837)}\n",
      "2022-05-02 16:37:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:13 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:15 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:15 [scrapy.extensions.telnet] INFO: Telnet Password: bfabe231a868f36d\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:26 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/St Malo_detailed.json\n",
      "2022-05-02 16:37:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24830,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6213608,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.250136,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 26, 792466),\n",
      " 'httpcompression/response_bytes': 36541618,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 15, 542330)}\n",
      "2022-05-02 16:37:26 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:28 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:28 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:28 [scrapy.extensions.telnet] INFO: Telnet Password: 716ddb0f2d14073c\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:28 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:39 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Strasbourg_detailed.json\n",
      "2022-05-02 16:37:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25086,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5968542,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.433494,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 39, 92979),\n",
      " 'httpcompression/response_bytes': 37179393,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 28, 659485)}\n",
      "2022-05-02 16:37:39 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:39 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:40 [scrapy.extensions.telnet] INFO: Telnet Password: 252be7805c3dd0ba\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:40 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:51 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Toulouse_detailed.json\n",
      "2022-05-02 16:37:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27601,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6310067,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 10.920121,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 51, 938052),\n",
      " 'httpcompression/response_bytes': 38477058,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 41, 17931)}\n",
      "2022-05-02 16:37:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:53 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:53 [scrapy.extensions.telnet] INFO: Telnet Password: 000ac206b638ebe6\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:38:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:38:03 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Uzes_detailed.json\n",
      "2022-05-02 16:38:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24925,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5678687,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.565694,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 38, 3, 499541),\n",
      " 'httpcompression/response_bytes': 33108450,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 53, 933847)}\n",
      "2022-05-02 16:38:03 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:38:03 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:38:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:38:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n"
     ]
    }
   ],
   "source": [
    "#we launched the detailed spiders, getting results for each hotel (gps and description). Spider is in bookingindividual.py\n",
    "pathresults= os.path.dirname(os.getcwd())+\"\\\\main\\\\results\"\n",
    "for file in os.listdir(pathresults):\n",
    "    #we create an environ variable to path the list of files to the spider\n",
    "    os.environ[\"files\"]=str(pathresults+\"\\\\\"+file)\n",
    "    !scrapy crawl bookingindividual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all files from the first spider\n",
    "\n",
    "\n",
    "#create an empty dataframe\n",
    "dataframemain= pd.DataFrame(columns = [\"name\",\"url\",\"Rating\"])\n",
    "\n",
    "\n",
    "pathresults= os.path.dirname(os.getcwd())+\"\\\\main\\\\results\"\n",
    "for file in os.listdir(pathresults):\n",
    "    with open(os.path.dirname(os.getcwd())+\"\\\\main\\\\results\\\\\"+file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        dataframemain=pd.concat([dataframemain,pd.DataFrame(data)])\n",
    "\n",
    "dataframemain.describe(include=\"all\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar process for the detailed resulsts of the second spider, Creating an empty dataframe\n",
    "dataframedetailed= pd.DataFrame(columns = [\"url\",\"description\",\"bbox\",\"Destination\"])\n",
    "\n",
    "\n",
    "pathresults= os.getcwd()+\"\\\\resultsdetailed\"\n",
    "for file in os.listdir(pathresults):\n",
    "    with open(pathresults+\"\\\\\"+file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        dataframedetailed=pd.concat([dataframedetailed,pd.DataFrame(data)])\n",
    "\n",
    "dataframedetailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a part of the url is missing from the second spider, so adding it\n",
    "dataframedetailed[\"url\"]=dataframedetailed[\"url\"].apply(lambda x: x+\"#hotelTmpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the url are similar, we can merge the 2 tables\n",
    "dfbbox=pd.merge(dataframemain,dataframedetailed,on=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>Rating</th>\n",
       "      <th>description</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "      <td>861</td>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>875</td>\n",
       "      <td>883</td>\n",
       "      <td>48</td>\n",
       "      <td>877</td>\n",
       "      <td>875</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hôtel Central</td>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>8,1</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>7.33500722464803,48.0601503371431,7.3888857086...</td>\n",
       "      <td>Toulouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                url  \\\n",
       "count             883                                                883   \n",
       "unique            875                                                883   \n",
       "top     Hôtel Central  https://www.booking.com/hotel/fr/canal-aigues-...   \n",
       "freq                2                                                  1   \n",
       "\n",
       "       Rating                                        description  \\\n",
       "count     861                                                883   \n",
       "unique     48                                                877   \n",
       "top       8,1  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "freq       48                                                  2   \n",
       "\n",
       "                                                     bbox Destination  \n",
       "count                                                 883         883  \n",
       "unique                                                875          35  \n",
       "top     7.33500722464803,48.0601503371431,7.3888857086...    Toulouse  \n",
       "freq                                                    2          27  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbbox.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert bbox to gps. It's 2 set of lat and longitudes, so taking the average to have the center of the hotel instead of its overall area\n",
    "dfbbox[\"Lat\"]=dfbbox[\"bbox\"].apply(lambda x: float(x.split(\",\")[1])/2+float(x.split(\",\")[3])/2)\n",
    "dfbbox[\"Lon\"]=dfbbox[\"bbox\"].apply(lambda x: float(x.split(\",\")[0])/2+float(x.split(\",\")[2])/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>Rating</th>\n",
       "      <th>description</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hotel Canal Aigues Mortes</td>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>8,6</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16998185644106,43.5550221035115,4.2196675347...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.572988</td>\n",
       "      <td>4.194825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La Villa Mazarin</td>\n",
       "      <td>https://www.booking.com/hotel/fr/la-villa-maza...</td>\n",
       "      <td>9,1</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16691241748164,43.5470212030719,4.2165914962...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.564987</td>\n",
       "      <td>4.191752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maison des Croisades</td>\n",
       "      <td>https://www.booking.com/hotel/fr/des-croisades...</td>\n",
       "      <td>8,9</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.1634570066567,43.5510319834269,4.21313939334...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.568997</td>\n",
       "      <td>4.188298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hôtel Saint Louis</td>\n",
       "      <td>https://www.booking.com/hotel/fr/saint-louis-a...</td>\n",
       "      <td>8,4</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16450314127171,43.549254254998,4.21418406165...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.567220</td>\n",
       "      <td>4.189344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Noemys Aigues-Mortes - ex Mona Lisa Royal Hôtel</td>\n",
       "      <td>https://www.booking.com/hotel/fr/le-royal-hote...</td>\n",
       "      <td>6,9</td>\n",
       "      <td>[Featuring free WiFi and a seasonal outdoor sw...</td>\n",
       "      <td>4.17297379558578,43.5584304713006,4.2226622861...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.576396</td>\n",
       "      <td>4.197818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  \\\n",
       "0                        Hotel Canal Aigues Mortes   \n",
       "1                                 La Villa Mazarin   \n",
       "2                             Maison des Croisades   \n",
       "3                                Hôtel Saint Louis   \n",
       "4  Noemys Aigues-Mortes - ex Mona Lisa Royal Hôtel   \n",
       "\n",
       "                                                 url Rating  \\\n",
       "0  https://www.booking.com/hotel/fr/canal-aigues-...    8,6   \n",
       "1  https://www.booking.com/hotel/fr/la-villa-maza...    9,1   \n",
       "2  https://www.booking.com/hotel/fr/des-croisades...    8,9   \n",
       "3  https://www.booking.com/hotel/fr/saint-louis-a...    8,4   \n",
       "4  https://www.booking.com/hotel/fr/le-royal-hote...    6,9   \n",
       "\n",
       "                                         description  \\\n",
       "0  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "1  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "2  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "3  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "4  [Featuring free WiFi and a seasonal outdoor sw...   \n",
       "\n",
       "                                                bbox    Destination  \\\n",
       "0  4.16998185644106,43.5550221035115,4.2196675347...  Aigues Mortes   \n",
       "1  4.16691241748164,43.5470212030719,4.2165914962...  Aigues Mortes   \n",
       "2  4.1634570066567,43.5510319834269,4.21313939334...  Aigues Mortes   \n",
       "3  4.16450314127171,43.549254254998,4.21418406165...  Aigues Mortes   \n",
       "4  4.17297379558578,43.5584304713006,4.2226622861...  Aigues Mortes   \n",
       "\n",
       "         Lat       Lon  \n",
       "0  43.572988  4.194825  \n",
       "1  43.564987  4.191752  \n",
       "2  43.568997  4.188298  \n",
       "3  43.567220  4.189344  \n",
       "4  43.576396  4.197818  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbbox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going back to kayak project and exporting the file\n",
    "os.chdir(\"..\")\n",
    "\n",
    "dfbbox.to_csv(\"Hotels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8af8a5d58c03aea7fed271afda0c18009a73827ffd40ee9cf6d51ffb766ed25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
