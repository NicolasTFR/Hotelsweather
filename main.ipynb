{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_destination=[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\main'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"main\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 16:14:46 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:14:46 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:14:46 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:14:46 [scrapy.extensions.telnet] INFO: Telnet Password: fff953bbd5cb1343\n",
      "2022-05-02 16:14:46 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:14:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:14:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:14:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:14:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:14:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:14:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:14:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:14:51 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: results\\Mont Saint Michel_main.json\n",
      "2022-05-02 16:14:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2111,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311191,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.794808,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 14, 51, 249708),\n",
      " 'httpcompression/response_bytes': 1955859,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 36,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 14, 47, 454900)}\n",
      "2022-05-02 16:14:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:14:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:14:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:14:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:14:52 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:14:52 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:14:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:14:52 [scrapy.extensions.telnet] INFO: Telnet Password: 864fc1af058eca66\n",
      "2022-05-02 16:14:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:14:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:14:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:14:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:14:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:14:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:14:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:14:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:14:55 [scrapy.extensions.feedexport] INFO: Stored json feed (52 items) in: results\\St Malo_main.json\n",
      "2022-05-02 16:14:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2089,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305822,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.67999,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 14, 55, 833128),\n",
      " 'httpcompression/response_bytes': 1993890,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 52,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 14, 53, 153138)}\n",
      "2022-05-02 16:14:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:14:55 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:14:55 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:14:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:14:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:14:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:14:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:14:57 [scrapy.extensions.telnet] INFO: Telnet Password: 4a282cf4b0c890c8\n",
      "2022-05-02 16:14:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:14:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:14:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:14:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:14:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:14:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:14:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:00 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: results\\Bayeux_main.json\n",
      "2022-05-02 16:15:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310013,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.20621,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 0, 881696),\n",
      " 'httpcompression/response_bytes': 1936166,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 36,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 14, 57, 675486)}\n",
      "2022-05-02 16:15:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:02 [scrapy.extensions.telnet] INFO: Telnet Password: 0459df67e292a803\n",
      "2022-05-02 16:15:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:05 [scrapy.extensions.feedexport] INFO: Stored json feed (62 items) in: results\\Le Havre_main.json\n",
      "2022-05-02 16:15:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305394,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.235008,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 5, 931406),\n",
      " 'httpcompression/response_bytes': 1967691,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 62,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 2, 696398)}\n",
      "2022-05-02 16:15:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:05 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:05 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:07 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:07 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:07 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:07 [scrapy.extensions.telnet] INFO: Telnet Password: 7da87b7c43c56a5d\n",
      "2022-05-02 16:15:07 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:10 [scrapy.extensions.feedexport] INFO: Stored json feed (39 items) in: results\\Rouen_main.json\n",
      "2022-05-02 16:15:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2081,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305616,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.807014,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 10, 547513),\n",
      " 'httpcompression/response_bytes': 1934533,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 39,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 7, 740499)}\n",
      "2022-05-02 16:15:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:10 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:11 [scrapy.extensions.telnet] INFO: Telnet Password: 326c882fcd028829\n",
      "2022-05-02 16:15:11 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:15 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:15 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: results\\Paris_main.json\n",
      "2022-05-02 16:15:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2071,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306889,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.220929,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 15, 531217),\n",
      " 'httpcompression/response_bytes': 1993993,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 38,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 12, 310288)}\n",
      "2022-05-02 16:15:15 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:15 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:16 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:16 [scrapy.extensions.telnet] INFO: Telnet Password: 338c15b6483f0540\n",
      "2022-05-02 16:15:16 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:17 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:17 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:17 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:17 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:17 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:17 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:20 [scrapy.extensions.feedexport] INFO: Stored json feed (43 items) in: results\\Amiens_main.json\n",
      "2022-05-02 16:15:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310871,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.184356,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 20, 514799),\n",
      " 'httpcompression/response_bytes': 1949263,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 43,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 17, 330443)}\n",
      "2022-05-02 16:15:20 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:21 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:21 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:21 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:21 [scrapy.extensions.telnet] INFO: Telnet Password: e70ec69b9aa88f9a\n",
      "2022-05-02 16:15:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:25 [scrapy.extensions.feedexport] INFO: Stored json feed (44 items) in: results\\Lille_main.json\n",
      "2022-05-02 16:15:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2065,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310799,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.046417,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 25, 392186),\n",
      " 'httpcompression/response_bytes': 1995010,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 44,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 22, 345769)}\n",
      "2022-05-02 16:15:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:25 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:26 [scrapy.extensions.telnet] INFO: Telnet Password: f9b6f86d4b55744b\n",
      "2022-05-02 16:15:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:27 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:27 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:27 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:27 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:30 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:30 [scrapy.extensions.feedexport] INFO: Stored json feed (45 items) in: results\\Strasbourg_main.json\n",
      "2022-05-02 16:15:30 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2104,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309792,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.988079,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 30, 218402),\n",
      " 'httpcompression/response_bytes': 2003512,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 45,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 27, 230323)}\n",
      "2022-05-02 16:15:30 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:30 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:30 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:30 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:31 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:31 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:31 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:31 [scrapy.extensions.telnet] INFO: Telnet Password: a8b18d6150b7188e\n",
      "2022-05-02 16:15:31 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:31 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:31 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:35 [scrapy.extensions.feedexport] INFO: Stored json feed (36 items) in: results\\Chateau du Haut Koenigsbourg_main.json\n",
      "2022-05-02 16:15:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2134,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309538,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.288249,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 35, 329634),\n",
      " 'httpcompression/response_bytes': 1960953,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 36,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 32, 41385)}\n",
      "2022-05-02 16:15:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:36 [scrapy.extensions.telnet] INFO: Telnet Password: 674e252ea2d03997\n",
      "2022-05-02 16:15:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:40 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Colmar_main.json\n",
      "2022-05-02 16:15:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2072,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305607,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.173749,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 40, 348004),\n",
      " 'httpcompression/response_bytes': 1947695,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 37, 174255)}\n",
      "2022-05-02 16:15:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:41 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:41 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:41 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:41 [scrapy.extensions.telnet] INFO: Telnet Password: dddd4cdb2d7e7dc0\n",
      "2022-05-02 16:15:41 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:41 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:45 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Eguisheim_main.json\n",
      "2022-05-02 16:15:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2083,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311954,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.662409,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 45, 867987),\n",
      " 'httpcompression/response_bytes': 1994953,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 42, 205578)}\n",
      "2022-05-02 16:15:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:47 [scrapy.extensions.telnet] INFO: Telnet Password: e067d1402984973c\n",
      "2022-05-02 16:15:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:47 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:47 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:47 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:47 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:47 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:47 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:50 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Besancon_main.json\n",
      "2022-05-02 16:15:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2098,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307572,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.04897,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 50, 874512),\n",
      " 'httpcompression/response_bytes': 1936564,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 47, 825542)}\n",
      "2022-05-02 16:15:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:50 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:52 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:52 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:52 [scrapy.extensions.telnet] INFO: Telnet Password: 59b7e3bdcadb52ed\n",
      "2022-05-02 16:15:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:15:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:15:55 [scrapy.extensions.feedexport] INFO: Stored json feed (41 items) in: results\\Dijon_main.json\n",
      "2022-05-02 16:15:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2083,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 305183,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.167756,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 15, 55, 856110),\n",
      " 'httpcompression/response_bytes': 1956927,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 41,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 52, 688354)}\n",
      "2022-05-02 16:15:55 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:15:55 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:15:55 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:15:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:15:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:15:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:15:57 [scrapy.extensions.telnet] INFO: Telnet Password: 31e08d7e1d3cb24b\n",
      "2022-05-02 16:15:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:15:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:15:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:15:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:15:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:15:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:15:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:00 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Annecy_main.json\n",
      "2022-05-02 16:16:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2070,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 311126,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.056939,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 0, 710050),\n",
      " 'httpcompression/response_bytes': 1983304,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 15, 57, 653111)}\n",
      "2022-05-02 16:16:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:02 [scrapy.extensions.telnet] INFO: Telnet Password: 6444cca50f004818\n",
      "2022-05-02 16:16:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:02 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:02 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:02 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:02 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:02 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:02 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:05 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Grenoble_main.json\n",
      "2022-05-02 16:16:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 312045,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.850452,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 5, 453700),\n",
      " 'httpcompression/response_bytes': 1959018,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 2, 603248)}\n",
      "2022-05-02 16:16:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:05 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:05 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 16:16:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:06 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:06 [scrapy.extensions.telnet] INFO: Telnet Password: d87915bbef8b96fa\n",
      "2022-05-02 16:16:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:10 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: results\\Lyon_main.json\n",
      "2022-05-02 16:16:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2068,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310858,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.036924,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 10, 410995),\n",
      " 'httpcompression/response_bytes': 1968992,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 38,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 7, 374071)}\n",
      "2022-05-02 16:16:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:10 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:08 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:08 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:08 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:08 [scrapy.extensions.telnet] INFO: Telnet Password: 50e910f4274c04d7\n",
      "2022-05-02 16:16:08 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:08 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:08 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:08 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:08 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2022-05-02 16:16:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:12 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Gorges du Verdon_main.json\n",
      "2022-05-02 16:16:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2116,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309942,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.373052,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 12, 565677),\n",
      " 'httpcompression/response_bytes': 1935597,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 9, 192625)}\n",
      "2022-05-02 16:16:12 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:14 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:14 [scrapy.extensions.telnet] INFO: Telnet Password: c7addb6ab315a8e9\n",
      "2022-05-02 16:16:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:14 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:17 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:17 [scrapy.extensions.feedexport] INFO: Stored json feed (56 items) in: results\\Bormes les Mimosas_main.json\n",
      "2022-05-02 16:16:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2126,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 312627,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.338369,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 17, 931654),\n",
      " 'httpcompression/response_bytes': 2024852,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 56,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 14, 593285)}\n",
      "2022-05-02 16:16:17 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:17 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:17 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:17 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:19 [scrapy.extensions.telnet] INFO: Telnet Password: 05919c87ce434020\n",
      "2022-05-02 16:16:19 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:19 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:19 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:19 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:19 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:19 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:22 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:22 [scrapy.extensions.feedexport] INFO: Stored json feed (60 items) in: results\\Cassis_main.json\n",
      "2022-05-02 16:16:22 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2086,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 308659,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.247891,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 22, 938608),\n",
      " 'httpcompression/response_bytes': 2024023,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 60,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 19, 690717)}\n",
      "2022-05-02 16:16:22 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:22 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:24 [scrapy.extensions.telnet] INFO: Telnet Password: e4a8048c2df83c12\n",
      "2022-05-02 16:16:24 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:24 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:27 [scrapy.extensions.feedexport] INFO: Stored json feed (38 items) in: results\\Marseille_main.json\n",
      "2022-05-02 16:16:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2099,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 313474,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.134968,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 27, 844526),\n",
      " 'httpcompression/response_bytes': 2011711,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 38,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 24, 709558)}\n",
      "2022-05-02 16:16:27 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:27 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:27 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:27 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:29 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:29 [scrapy.extensions.telnet] INFO: Telnet Password: 3f84e01ece2e519c\n",
      "2022-05-02 16:16:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:29 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:29 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:29 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:29 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:32 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Aix en Provence_main.json\n",
      "2022-05-02 16:16:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2099,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309618,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.897835,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 32, 528462),\n",
      " 'httpcompression/response_bytes': 1966835,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 29, 630627)}\n",
      "2022-05-02 16:16:32 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:32 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:32 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:33 [scrapy.extensions.telnet] INFO: Telnet Password: 1863d6313de83cb8\n",
      "2022-05-02 16:16:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:34 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:34 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:34 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:34 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:37 [scrapy.extensions.feedexport] INFO: Stored json feed (42 items) in: results\\Avignon_main.json\n",
      "2022-05-02 16:16:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2091,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307677,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.970184,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 37, 427658),\n",
      " 'httpcompression/response_bytes': 1967071,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 34, 457474)}\n",
      "2022-05-02 16:16:37 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:37 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:37 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:38 [scrapy.extensions.telnet] INFO: Telnet Password: 94683991075c90c7\n",
      "2022-05-02 16:16:38 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:42 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:42 [scrapy.extensions.feedexport] INFO: Stored json feed (41 items) in: results\\Uzes_main.json\n",
      "2022-05-02 16:16:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2078,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 312463,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.290881,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 42, 706499),\n",
      " 'httpcompression/response_bytes': 1964962,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 41,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 39, 415618)}\n",
      "2022-05-02 16:16:42 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:42 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:42 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:44 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:44 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:44 [scrapy.extensions.telnet] INFO: Telnet Password: 28e359810f32cb5b\n",
      "2022-05-02 16:16:44 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:44 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:47 [scrapy.extensions.feedexport] INFO: Stored json feed (41 items) in: results\\Nimes_main.json\n",
      "2022-05-02 16:16:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2085,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 312742,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.209883,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 47, 802171),\n",
      " 'httpcompression/response_bytes': 2001162,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 41,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 44, 592288)}\n",
      "2022-05-02 16:16:47 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:49 [scrapy.extensions.telnet] INFO: Telnet Password: b068b7487aff1924\n",
      "2022-05-02 16:16:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:53 [scrapy.extensions.feedexport] INFO: Stored json feed (40 items) in: results\\Aigues Mortes_main.json\n",
      "2022-05-02 16:16:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2105,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309988,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.19128,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 53, 203082),\n",
      " 'httpcompression/response_bytes': 1919396,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 40,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 50, 11802)}\n",
      "2022-05-02 16:16:53 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:53 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:54 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:54 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:54 [scrapy.extensions.telnet] INFO: Telnet Password: 5ec47759ec365f9f\n",
      "2022-05-02 16:16:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:54 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:16:55 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:16:55 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:16:58 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:16:58 [scrapy.extensions.feedexport] INFO: Stored json feed (58 items) in: results\\Saintes Maries de la mer_main.json\n",
      "2022-05-02 16:16:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2124,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 309769,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.166391,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 16, 58, 222327),\n",
      " 'httpcompression/response_bytes': 1981775,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 58,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 16, 55, 55936)}\n",
      "2022-05-02 16:16:58 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:16:58 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:16:58 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:16:59 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:16:59 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:16:59 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:16:59 [scrapy.extensions.telnet] INFO: Telnet Password: 71b357ca2db279c1\n",
      "2022-05-02 16:16:59 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:16:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:16:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:16:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:16:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:03 [scrapy.extensions.feedexport] INFO: Stored json feed (63 items) in: results\\Collioure_main.json\n",
      "2022-05-02 16:17:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2087,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 313549,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.117108,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 3, 134856),\n",
      " 'httpcompression/response_bytes': 2030621,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 63,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 0, 17748)}\n",
      "2022-05-02 16:17:03 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:03 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:04 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:04 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:04 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:04 [scrapy.extensions.telnet] INFO: Telnet Password: 11495c848f80f060\n",
      "2022-05-02 16:17:04 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:08 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:08 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Carcassonne_main.json\n",
      "2022-05-02 16:17:08 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2105,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307401,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.253041,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 8, 157553),\n",
      " 'httpcompression/response_bytes': 1994380,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 4, 904512)}\n",
      "2022-05-02 16:17:08 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:08 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:08 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:08 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:09 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:09 [scrapy.extensions.telnet] INFO: Telnet Password: ef9f4b569aff5969\n",
      "2022-05-02 16:17:09 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:09 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:13 [scrapy.extensions.feedexport] INFO: Stored json feed (34 items) in: results\\Ariege_main.json\n",
      "2022-05-02 16:17:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2066,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310193,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.586521,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 13, 780028),\n",
      " 'httpcompression/response_bytes': 1950547,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 34,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 10, 193507)}\n",
      "2022-05-02 16:17:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:13 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:15 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:15 [scrapy.extensions.telnet] INFO: Telnet Password: 20979134dee4dc3b\n",
      "2022-05-02 16:17:15 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:18 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:18 [scrapy.extensions.feedexport] INFO: Stored json feed (42 items) in: results\\Toulouse_main.json\n",
      "2022-05-02 16:17:18 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2090,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 308657,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.963124,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 18, 614527),\n",
      " 'httpcompression/response_bytes': 2023449,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 15, 651403)}\n",
      "2022-05-02 16:17:18 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:18 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:18 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:18 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:20 [scrapy.extensions.telnet] INFO: Telnet Password: 79bac78e2e31f79b\n",
      "2022-05-02 16:17:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:23 [scrapy.extensions.feedexport] INFO: Stored json feed (35 items) in: results\\Montauban_main.json\n",
      "2022-05-02 16:17:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2071,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307624,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.061407,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 23, 554209),\n",
      " 'httpcompression/response_bytes': 1906743,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 35,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 20, 492802)}\n",
      "2022-05-02 16:17:23 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:23 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:23 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:24 [scrapy.extensions.telnet] INFO: Telnet Password: db457af1998109dd\n",
      "2022-05-02 16:17:24 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:28 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:28 [scrapy.extensions.feedexport] INFO: Stored json feed (60 items) in: results\\Biarritz_main.json\n",
      "2022-05-02 16:17:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2082,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 306226,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.14632,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 28, 616783),\n",
      " 'httpcompression/response_bytes': 2009224,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 60,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 25, 470463)}\n",
      "2022-05-02 16:17:28 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:28 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:28 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:29 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:29 [scrapy.extensions.telnet] INFO: Telnet Password: 442d2f9762a286e1\n",
      "2022-05-02 16:17:29 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:30 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:33 [scrapy.extensions.feedexport] INFO: Stored json feed (37 items) in: results\\Bayonne_main.json\n",
      "2022-05-02 16:17:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2097,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 307161,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.215959,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 33, 599796),\n",
      " 'httpcompression/response_bytes': 1958389,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 37,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 30, 383837)}\n",
      "2022-05-02 16:17:33 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:17:34 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:17:34 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:34 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:17:34 [scrapy.extensions.telnet] INFO: Telnet Password: fb4f7467d5b48bd1\n",
      "2022-05-02 16:17:34 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:17:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:17:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:17:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:17:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:17:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:17:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:17:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:17:38 [scrapy.extensions.feedexport] INFO: Stored json feed (42 items) in: results\\La Rochelle_main.json\n",
      "2022-05-02 16:17:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 2109,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 310553,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 3.115062,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 17, 38, 474199),\n",
      " 'httpcompression/response_bytes': 1962253,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'item_scraped_count': 42,\n",
      " 'log_count/INFO': 11,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 17, 35, 359137)}\n",
      "2022-05-02 16:17:38 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:17:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:17:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:17:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n"
     ]
    }
   ],
   "source": [
    "for destination in list_destination:\n",
    "    os.environ[\"listdestination\"]=destination\n",
    "    !scrapy crawl bookingmain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nones(value):\n",
    "    \"\"\"\n",
    "    Recursively remove all None values from dictionaries and lists, and returns\n",
    "    the result as a new dictionary or list.\n",
    "    \"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return [clean_nones(x) for x in value if x is not None]\n",
    "    elif isinstance(value, dict):\n",
    "        return {\n",
    "            key: clean_nones(val)\n",
    "            for key, val in value.items()\n",
    "            if val is not None\n",
    "        }\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\main\\\\results'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in os.listdir():\n",
    "    with open(files) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        datacleaned=clean_nones(data)\n",
    "        datacleaned2=list(filter(None, datacleaned))\n",
    "        with open(files, 'w') as f:\n",
    "            json.dump(datacleaned2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\main'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"detailed\")\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\detailed'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 16:30:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:30:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:30:45 [scrapy.extensions.telnet] INFO: Telnet Password: 6c5471312aaad27d\n",
      "2022-05-02 16:30:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:30:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:30:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:30:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:30:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:30:56 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:30:56 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Aigues Mortes_detailed.json\n",
      "2022-05-02 16:30:56 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24930,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5622324,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.024687,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 30, 56, 310470),\n",
      " 'httpcompression/response_bytes': 32819535,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 30, 46, 285783)}\n",
      "2022-05-02 16:30:56 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:30:56 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:30:56 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:56 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:30:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:30:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:30:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:30:57 [scrapy.extensions.telnet] INFO: Telnet Password: 6756204860aafdd7\n",
      "2022-05-02 16:30:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:30:58 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:30:58 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:30:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:30:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:09 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Aix en Provence_detailed.json\n",
      "2022-05-02 16:31:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24911,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5843812,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.142214,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 9, 472531),\n",
      " 'httpcompression/response_bytes': 35542418,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 30, 58, 330317)}\n",
      "2022-05-02 16:31:09 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:09 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:09 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:09 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:10 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:10 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:10 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:10 [scrapy.extensions.telnet] INFO: Telnet Password: 877eb4dbeec4073b\n",
      "2022-05-02 16:31:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:11 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:11 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:11 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:11 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:20 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:20 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Amiens_detailed.json\n",
      "2022-05-02 16:31:20 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24982,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5684188,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.609137,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 20, 953028),\n",
      " 'httpcompression/response_bytes': 33519655,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 11, 343891)}\n",
      "2022-05-02 16:31:20 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:22 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:22 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:22 [scrapy.extensions.telnet] INFO: Telnet Password: 6873d1941966a4eb\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:33 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:33 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Annecy_detailed.json\n",
      "2022-05-02 16:31:33 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24991,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5823304,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.8725,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 33, 689023),\n",
      " 'httpcompression/response_bytes': 35556367,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 22, 816523)}\n",
      "2022-05-02 16:31:33 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:35 [scrapy.extensions.telnet] INFO: Telnet Password: 91e76f4676bb53ff\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:35 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:45 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Ariege_detailed.json\n",
      "2022-05-02 16:31:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24930,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5678429,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.283151,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 45, 914972),\n",
      " 'httpcompression/response_bytes': 33385939,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 35, 631821)}\n",
      "2022-05-02 16:31:45 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:31:47 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:31:47 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:47 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:31:47 [scrapy.extensions.telnet] INFO: Telnet Password: e70a3e95cb2d0d88\n",
      "2022-05-02 16:31:47 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:31:48 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:31:48 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:31:48 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:31:48 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:31:59 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:31:59 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Avignon_detailed.json\n",
      "2022-05-02 16:31:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24965,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5926076,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.006819,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 31, 59, 337442),\n",
      " 'httpcompression/response_bytes': 38008892,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 31, 48, 330623)}\n",
      "2022-05-02 16:31:59 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:31:59 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:31:59 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:31:59 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:00 [scrapy.extensions.telnet] INFO: Telnet Password: f23c7d8285b44c9c\n",
      "2022-05-02 16:32:00 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:11 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:11 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bayeux_detailed.json\n",
      "2022-05-02 16:32:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24805,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5808008,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.060355,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 11, 269894),\n",
      " 'httpcompression/response_bytes': 35552099,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 1, 209539)}\n",
      "2022-05-02 16:32:11 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:12 [scrapy.extensions.telnet] INFO: Telnet Password: 848dba20b5520c42\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:12 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:23 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bayonne_detailed.json\n",
      "2022-05-02 16:32:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24867,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5707761,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.983078,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 23, 87524),\n",
      " 'httpcompression/response_bytes': 34140267,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 13, 104446)}\n",
      "2022-05-02 16:32:23 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:23 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:23 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:23 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:24 [scrapy.extensions.telnet] INFO: Telnet Password: b927994f8b70336a\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:24 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:35 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:35 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Besancon_detailed.json\n",
      "2022-05-02 16:32:35 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24884,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5811178,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.745808,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 35, 723073),\n",
      " 'httpcompression/response_bytes': 34551382,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 24, 977265)}\n",
      "2022-05-02 16:32:35 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:35 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:35 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:36 [scrapy.extensions.telnet] INFO: Telnet Password: 7d409fb34c7e0593\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:37 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:37 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:37 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:37 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:32:48 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:32:48 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Biarritz_detailed.json\n",
      "2022-05-02 16:32:48 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25134,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6314311,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.614111,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 32, 48, 51794),\n",
      " 'httpcompression/response_bytes': 38777450,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 37, 437683)}\n",
      "2022-05-02 16:32:48 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:32:48 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:32:48 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:48 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:32:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:32:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:32:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:32:49 [scrapy.extensions.telnet] INFO: Telnet Password: 4f50be37eee9c97b\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:32:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:32:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:32:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:32:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:00 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Bormes les Mimosas_detailed.json\n",
      "2022-05-02 16:33:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25175,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6165989,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.31939,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 0, 44808),\n",
      " 'httpcompression/response_bytes': 38179882,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 32, 49, 725418)}\n",
      "2022-05-02 16:33:00 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:00 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:00 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:00 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:01 [scrapy.extensions.telnet] INFO: Telnet Password: d80d90a5f6463caf\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:01 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:01 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:01 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:01 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:13 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Carcassonne_detailed.json\n",
      "2022-05-02 16:33:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27393,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6315637,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 11.197834,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 13, 47847),\n",
      " 'httpcompression/response_bytes': 39878341,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 1, 850013)}\n",
      "2022-05-02 16:33:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:13 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:14 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:14 [scrapy.extensions.telnet] INFO: Telnet Password: b60b461441027289\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:14 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:25 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:25 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Cassis_detailed.json\n",
      "2022-05-02 16:33:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25506,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6097021,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.626818,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 25, 352824),\n",
      " 'httpcompression/response_bytes': 35430816,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 14, 726006)}\n",
      "2022-05-02 16:33:25 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:25 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:26 [scrapy.extensions.telnet] INFO: Telnet Password: 5ad6ed4da002b27c\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:38 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:38 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Chateau du Haut Koenigsbourg_detailed.json\n",
      "2022-05-02 16:33:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24818,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5865589,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.024899,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 38, 53632),\n",
      " 'httpcompression/response_bytes': 35961630,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 27, 28733)}\n",
      "2022-05-02 16:33:38 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:39 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:39 [scrapy.extensions.telnet] INFO: Telnet Password: 55cbdf8ec2f1ce76\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:33:50 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:33:50 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Collioure_detailed.json\n",
      "2022-05-02 16:33:50 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25311,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6092438,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.823191,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 33, 50, 586849),\n",
      " 'httpcompression/response_bytes': 36878528,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 39, 763658)}\n",
      "2022-05-02 16:33:50 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:33:50 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:33:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:33:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:33:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:33:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:33:51 [scrapy.extensions.telnet] INFO: Telnet Password: 0bad3304765272c8\n",
      "2022-05-02 16:33:51 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:33:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:33:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:33:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:33:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:03 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Colmar_detailed.json\n",
      "2022-05-02 16:34:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24929,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5806109,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.029488,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 3, 306885),\n",
      " 'httpcompression/response_bytes': 35034620,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 33, 52, 277397)}\n",
      "2022-05-02 16:34:03 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:03 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:04 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:04 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:04 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:04 [scrapy.extensions.telnet] INFO: Telnet Password: 26da2104340f86c6\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:05 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:16 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:16 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Dijon_detailed.json\n",
      "2022-05-02 16:34:16 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24853,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5913873,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.261334,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 16, 430800),\n",
      " 'httpcompression/response_bytes': 37418300,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 5, 169466)}\n",
      "2022-05-02 16:34:16 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:16 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:16 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:16 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:17 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:17 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:17 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:17 [scrapy.extensions.telnet] INFO: Telnet Password: c283dcb80c67a5a9\n",
      "2022-05-02 16:34:17 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:18 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:18 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:18 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:18 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:28 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:28 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Eguisheim_detailed.json\n",
      "2022-05-02 16:34:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24990,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5754106,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.395015,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 28, 686624),\n",
      " 'httpcompression/response_bytes': 33955889,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 18, 291609)}\n",
      "2022-05-02 16:34:28 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:28 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:28 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:29 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:29 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:29 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:30 [scrapy.extensions.telnet] INFO: Telnet Password: 48a66e1bf6df04c8\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:30 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:30 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:30 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:40 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Gorges du Verdon_detailed.json\n",
      "2022-05-02 16:34:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24928,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5723009,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.35252,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 40, 774978),\n",
      " 'httpcompression/response_bytes': 35249950,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 30, 422458)}\n",
      "2022-05-02 16:34:40 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:42 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:42 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:42 [scrapy.extensions.telnet] INFO: Telnet Password: 8af9aeaf38793693\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:34:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:34:53 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Grenoble_detailed.json\n",
      "2022-05-02 16:34:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24906,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5860228,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.02558,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 34, 53, 139675),\n",
      " 'httpcompression/response_bytes': 34823553,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 43, 114095)}\n",
      "2022-05-02 16:34:53 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:34:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:34:53 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:34:54 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:34:54 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:34:54 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:34:54 [scrapy.extensions.telnet] INFO: Telnet Password: 836f0d30c097bd56\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:34:54 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:34:54 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:34:54 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:34:54 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:05 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/La Rochelle_detailed.json\n",
      "2022-05-02 16:35:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25039,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5855300,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.708969,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 5, 624440),\n",
      " 'httpcompression/response_bytes': 35996640,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 34, 54, 915471)}\n",
      "2022-05-02 16:35:05 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:05 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:05 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:06 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:06 [scrapy.extensions.telnet] INFO: Telnet Password: 0798ce6251fe9a4b\n",
      "2022-05-02 16:35:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:07 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:07 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:19 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:19 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Le Havre_detailed.json\n",
      "2022-05-02 16:35:19 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25396,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6224853,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.854452,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 19, 128756),\n",
      " 'httpcompression/response_bytes': 36973944,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 7, 274304)}\n",
      "2022-05-02 16:35:19 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:19 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:19 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:19 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:20 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:20 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:20 [scrapy.extensions.telnet] INFO: Telnet Password: b26915a288ce79f5\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:20 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:20 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:20 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:20 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:32 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Lille_detailed.json\n",
      "2022-05-02 16:35:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25072,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5911274,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.311306,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 32, 86134),\n",
      " 'httpcompression/response_bytes': 36294029,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 20, 774828)}\n",
      "2022-05-02 16:35:32 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:32 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:32 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:32 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:33 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:33 [scrapy.extensions.telnet] INFO: Telnet Password: 276de36171cca6cf\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:33 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:34 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:34 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:44 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:44 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Lyon_detailed.json\n",
      "2022-05-02 16:35:44 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24947,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5797289,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.155747,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 44, 340926),\n",
      " 'httpcompression/response_bytes': 34648116,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 34, 185179)}\n",
      "2022-05-02 16:35:44 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:44 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:44 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:44 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:45 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:45 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:45 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:45 [scrapy.extensions.telnet] INFO: Telnet Password: 318d2fac28f08e9a\n",
      "2022-05-02 16:35:45 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:46 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:46 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:46 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:46 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:35:57 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:35:57 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Marseille_detailed.json\n",
      "2022-05-02 16:35:57 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27666,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6223310,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 11.259051,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 35, 57, 493127),\n",
      " 'httpcompression/response_bytes': 37318376,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 46, 234076)}\n",
      "2022-05-02 16:35:57 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:35:57 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:35:57 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:35:58 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:35:58 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:35:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:35:58 [scrapy.extensions.telnet] INFO: Telnet Password: c140ab543725b78c\n",
      "2022-05-02 16:35:58 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:35:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:35:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:35:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:35:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:11 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:11 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Mont Saint Michel_detailed.json\n",
      "2022-05-02 16:36:11 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 26066,\n",
      " 'downloader/request_count': 26,\n",
      " 'downloader/request_method_count/GET': 26,\n",
      " 'downloader/response_bytes': 6065378,\n",
      " 'downloader/response_count': 26,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'downloader/response_status_count/500': 1,\n",
      " 'elapsed_time_seconds': 12.10855,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 11, 470431),\n",
      " 'httpcompression/response_bytes': 32715322,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'retry/count': 1,\n",
      " 'retry/reason_count/500 Internal Server Error': 1,\n",
      " 'scheduler/dequeued': 26,\n",
      " 'scheduler/dequeued/memory': 26,\n",
      " 'scheduler/enqueued': 26,\n",
      " 'scheduler/enqueued/memory': 26,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 35, 59, 361881)}\n",
      "2022-05-02 16:36:11 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:11 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:11 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:11 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:12 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:12 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:12 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:12 [scrapy.extensions.telnet] INFO: Telnet Password: f16ab49b2f1c82c5\n",
      "2022-05-02 16:36:12 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:23 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:23 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Montauban_detailed.json\n",
      "2022-05-02 16:36:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24858,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5652581,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.678347,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 23, 977527),\n",
      " 'httpcompression/response_bytes': 32822271,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 13, 299180)}\n",
      "2022-05-02 16:36:23 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:24 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:24 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:24 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:25 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:25 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:25 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:25 [scrapy.extensions.telnet] INFO: Telnet Password: 3d4803bf746ca69d\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:25 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:25 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:36 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Nimes_detailed.json\n",
      "2022-05-02 16:36:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27578,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6222412,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 10.941203,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 36, 739491),\n",
      " 'httpcompression/response_bytes': 37343266,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 25, 798288)}\n",
      "2022-05-02 16:36:36 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:36 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:36 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:36 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:38 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:38 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:38 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:38 [scrapy.extensions.telnet] INFO: Telnet Password: bfc91e13a159c2bb\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:38 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:38 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:36:49 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:36:49 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Paris_detailed.json\n",
      "2022-05-02 16:36:49 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24709,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5771755,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.334221,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 36, 49, 57433),\n",
      " 'httpcompression/response_bytes': 34592765,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 38, 723212)}\n",
      "2022-05-02 16:36:49 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:36:49 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:36:49 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:49 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:36:50 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:36:50 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:36:50 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:36:50 [scrapy.extensions.telnet] INFO: Telnet Password: ee03434bc74424ab\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:36:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:36:50 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:36:50 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:36:50 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:01 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Rouen_detailed.json\n",
      "2022-05-02 16:37:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24990,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5734031,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.392263,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 1, 360796),\n",
      " 'httpcompression/response_bytes': 34223928,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 36, 50, 968533)}\n",
      "2022-05-02 16:37:01 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:01 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:01 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:01 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:02 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:02 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:02 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:02 [scrapy.extensions.telnet] INFO: Telnet Password: 13d4c2331262245d\n",
      "2022-05-02 16:37:02 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:03 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:03 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:13 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:13 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Saintes Maries de la mer_detailed.json\n",
      "2022-05-02 16:37:13 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25275,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6160184,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.460193,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 13, 686030),\n",
      " 'httpcompression/response_bytes': 38407198,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 3, 225837)}\n",
      "2022-05-02 16:37:13 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:13 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:13 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:15 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:15 [scrapy.extensions.telnet] INFO: Telnet Password: bfabe231a868f36d\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:15 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:15 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:15 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:15 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:26 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/St Malo_detailed.json\n",
      "2022-05-02 16:37:26 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24830,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 6213608,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 11.250136,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 26, 792466),\n",
      " 'httpcompression/response_bytes': 36541618,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 15, 542330)}\n",
      "2022-05-02 16:37:26 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:26 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:28 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:28 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:28 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:28 [scrapy.extensions.telnet] INFO: Telnet Password: 716ddb0f2d14073c\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:28 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:39 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Strasbourg_detailed.json\n",
      "2022-05-02 16:37:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 25086,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5968542,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 10.433494,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 39, 92979),\n",
      " 'httpcompression/response_bytes': 37179393,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 28, 659485)}\n",
      "2022-05-02 16:37:39 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:39 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:39 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:39 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:40 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:40 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:40 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:40 [scrapy.extensions.telnet] INFO: Telnet Password: 252be7805c3dd0ba\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:40 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:40 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:41 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:37:51 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:37:51 [scrapy.extensions.feedexport] INFO: Stored json feed (27 items) in: resultsdetailed/Toulouse_detailed.json\n",
      "2022-05-02 16:37:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 27601,\n",
      " 'downloader/request_count': 27,\n",
      " 'downloader/request_method_count/GET': 27,\n",
      " 'downloader/response_bytes': 6310067,\n",
      " 'downloader/response_count': 27,\n",
      " 'downloader/response_status_count/200': 27,\n",
      " 'elapsed_time_seconds': 10.920121,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 37, 51, 938052),\n",
      " 'httpcompression/response_bytes': 38477058,\n",
      " 'httpcompression/response_count': 27,\n",
      " 'item_scraped_count': 27,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 27,\n",
      " 'scheduler/dequeued': 27,\n",
      " 'scheduler/dequeued/memory': 27,\n",
      " 'scheduler/enqueued': 27,\n",
      " 'scheduler/enqueued/memory': 27,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 41, 17931)}\n",
      "2022-05-02 16:37:51 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:37:51 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:37:51 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:51 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n",
      "2022-05-02 16:37:53 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-05-02 16:37:53 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:37:53 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20, 'USER_AGENT': 'Chrome/97.0'}\n",
      "2022-05-02 16:37:53 [scrapy.extensions.telnet] INFO: Telnet Password: 000ac206b638ebe6\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-02 16:37:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-02 16:37:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-02 16:37:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-02 16:37:53 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-02 16:38:03 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-05-02 16:38:03 [scrapy.extensions.feedexport] INFO: Stored json feed (25 items) in: resultsdetailed/Uzes_detailed.json\n",
      "2022-05-02 16:38:03 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 24925,\n",
      " 'downloader/request_count': 25,\n",
      " 'downloader/request_method_count/GET': 25,\n",
      " 'downloader/response_bytes': 5678687,\n",
      " 'downloader/response_count': 25,\n",
      " 'downloader/response_status_count/200': 25,\n",
      " 'elapsed_time_seconds': 9.565694,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 5, 2, 14, 38, 3, 499541),\n",
      " 'httpcompression/response_bytes': 33108450,\n",
      " 'httpcompression/response_count': 25,\n",
      " 'item_scraped_count': 25,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 25,\n",
      " 'scheduler/dequeued': 25,\n",
      " 'scheduler/dequeued/memory': 25,\n",
      " 'scheduler/enqueued': 25,\n",
      " 'scheduler/enqueued/memory': 25,\n",
      " 'start_time': datetime.datetime(2022, 5, 2, 14, 37, 53, 933847)}\n",
      "2022-05-02 16:38:03 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2022-05-02 16:38:03 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-02 16:38:03 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 3.4.8, Platform Windows-10-10.0.19043-SP0\n",
      "2022-05-02 16:38:03 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_ENABLED': True,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\Scripts\\scrapy.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 145, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 100, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 153, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 205, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 238, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 313, in _create_crawler\n",
      "    return Crawler(spidercls, self.settings, init_reactor=True)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 82, in __init__\n",
      "    default.install()\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\selectreactor.py\", line 194, in install\n",
      "    installReactor(reactor)\n",
      "  File \"C:\\Users\\nicol\\anaconda3\\lib\\site-packages\\twisted\\internet\\main.py\", line 32, in installReactor\n",
      "    raise error.ReactorAlreadyInstalledError(\"reactor already installed\")\n",
      "twisted.internet.error.ReactorAlreadyInstalledError: reactor already installed\n"
     ]
    }
   ],
   "source": [
    "pathresults= os.path.dirname(os.getcwd())+\"\\\\main\\\\results\"\n",
    "for file in os.listdir(pathresults):\n",
    "    os.environ[\"files\"]=str(pathresults+\"\\\\\"+file)\n",
    "    !scrapy crawl bookingindividual -o xxx.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\nicol\\\\Desktop\\\\Kayakproject\\\\detailed'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all files\n",
    "\n",
    "dataframemain= pd.DataFrame(columns = [\"name\",\"url\",\"Rating\"])\n",
    "\n",
    "\n",
    "pathresults= os.path.dirname(os.getcwd())+\"\\\\main\\\\results\"\n",
    "for file in os.listdir(pathresults):\n",
    "    with open(os.path.dirname(os.getcwd())+\"\\\\main\\\\results\\\\\"+file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        dataframemain=pd.concat([dataframemain,pd.DataFrame(data)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>875</td>\n",
       "      <td>883</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hôtel Central</td>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>8,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                url  \\\n",
       "count             883                                                883   \n",
       "unique            875                                                883   \n",
       "top     Hôtel Central  https://www.booking.com/hotel/fr/canal-aigues-...   \n",
       "freq                2                                                  1   \n",
       "\n",
       "       Rating  \n",
       "count     861  \n",
       "unique     48  \n",
       "top       8,1  \n",
       "freq       48  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframemain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframedetailed= pd.DataFrame(columns = [\"url\",\"description\",\"bbox\",\"Destination\"])\n",
    "\n",
    "\n",
    "pathresults= os.getcwd()+\"\\\\resultsdetailed\"\n",
    "for file in os.listdir(pathresults):\n",
    "    with open(pathresults+\"\\\\\"+file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        dataframedetailed=pd.concat([dataframedetailed,pd.DataFrame(data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.booking.com/hotel/fr/residence-oda...</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.15440270440472,43.5561285834269,4.2040892955...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.booking.com/hotel/fr/le-dit-vin-se...</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16427272565645,43.5488035834269,4.2139532743...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.booking.com/hotel/fr/le-royal-hote...</td>\n",
       "      <td>[Featuring free WiFi and a seasonal outdoor sw...</td>\n",
       "      <td>4.17297379558578,43.5584304713006,4.2226622861...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16998185644106,43.5550221035115,4.2196675347...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.booking.com/hotel/fr/saint-louis-a...</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16450314127171,43.549254254998,4.21418406165...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.booking.com/hotel/fr/residence-oda...   \n",
       "1  https://www.booking.com/hotel/fr/le-dit-vin-se...   \n",
       "2  https://www.booking.com/hotel/fr/le-royal-hote...   \n",
       "3  https://www.booking.com/hotel/fr/canal-aigues-...   \n",
       "4  https://www.booking.com/hotel/fr/saint-louis-a...   \n",
       "\n",
       "                                         description  \\\n",
       "0  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "1  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "2  [Featuring free WiFi and a seasonal outdoor sw...   \n",
       "3  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "4  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "\n",
       "                                                bbox    Destination  \n",
       "0  4.15440270440472,43.5561285834269,4.2040892955...  Aigues Mortes  \n",
       "1  4.16427272565645,43.5488035834269,4.2139532743...  Aigues Mortes  \n",
       "2  4.17297379558578,43.5584304713006,4.2226622861...  Aigues Mortes  \n",
       "3  4.16998185644106,43.5550221035115,4.2196675347...  Aigues Mortes  \n",
       "4  4.16450314127171,43.549254254998,4.21418406165...  Aigues Mortes  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframedetailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframedetailed[\"url\"]=dataframedetailed[\"url\"].apply(lambda x: x+\"#hotelTmpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbbox=pd.merge(dataframemain,dataframedetailed,on=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>Rating</th>\n",
       "      <th>description</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "      <td>861</td>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>875</td>\n",
       "      <td>883</td>\n",
       "      <td>48</td>\n",
       "      <td>877</td>\n",
       "      <td>875</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hôtel Central</td>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>8,1</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>7.33500722464803,48.0601503371431,7.3888857086...</td>\n",
       "      <td>Toulouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                                url  \\\n",
       "count             883                                                883   \n",
       "unique            875                                                883   \n",
       "top     Hôtel Central  https://www.booking.com/hotel/fr/canal-aigues-...   \n",
       "freq                2                                                  1   \n",
       "\n",
       "       Rating                                        description  \\\n",
       "count     861                                                883   \n",
       "unique     48                                                877   \n",
       "top       8,1  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "freq       48                                                  2   \n",
       "\n",
       "                                                     bbox Destination  \n",
       "count                                                 883         883  \n",
       "unique                                                875          35  \n",
       "top     7.33500722464803,48.0601503371431,7.3888857086...    Toulouse  \n",
       "freq                                                    2          27  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbbox.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covnert bbox to gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbbox[\"Lat\"]=dfbbox[\"bbox\"].apply(lambda x: float(x.split(\",\")[1])/2+float(x.split(\",\")[3])/2)\n",
    "dfbbox[\"Lon\"]=dfbbox[\"bbox\"].apply(lambda x: float(x.split(\",\")[0])/2+float(x.split(\",\")[2])/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>Rating</th>\n",
       "      <th>description</th>\n",
       "      <th>bbox</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hotel Canal Aigues Mortes</td>\n",
       "      <td>https://www.booking.com/hotel/fr/canal-aigues-...</td>\n",
       "      <td>8,6</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16998185644106,43.5550221035115,4.2196675347...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.572988</td>\n",
       "      <td>4.194825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La Villa Mazarin</td>\n",
       "      <td>https://www.booking.com/hotel/fr/la-villa-maza...</td>\n",
       "      <td>9,1</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16691241748164,43.5470212030719,4.2165914962...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.564987</td>\n",
       "      <td>4.191752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maison des Croisades</td>\n",
       "      <td>https://www.booking.com/hotel/fr/des-croisades...</td>\n",
       "      <td>8,9</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.1634570066567,43.5510319834269,4.21313939334...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.568997</td>\n",
       "      <td>4.188298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hôtel Saint Louis</td>\n",
       "      <td>https://www.booking.com/hotel/fr/saint-louis-a...</td>\n",
       "      <td>8,4</td>\n",
       "      <td>[Vous pouvez bénéficier d'une réduction Genius...</td>\n",
       "      <td>4.16450314127171,43.549254254998,4.21418406165...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.567220</td>\n",
       "      <td>4.189344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Noemys Aigues-Mortes - ex Mona Lisa Royal Hôtel</td>\n",
       "      <td>https://www.booking.com/hotel/fr/le-royal-hote...</td>\n",
       "      <td>6,9</td>\n",
       "      <td>[Featuring free WiFi and a seasonal outdoor sw...</td>\n",
       "      <td>4.17297379558578,43.5584304713006,4.2226622861...</td>\n",
       "      <td>Aigues Mortes</td>\n",
       "      <td>43.576396</td>\n",
       "      <td>4.197818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              name  \\\n",
       "0                        Hotel Canal Aigues Mortes   \n",
       "1                                 La Villa Mazarin   \n",
       "2                             Maison des Croisades   \n",
       "3                                Hôtel Saint Louis   \n",
       "4  Noemys Aigues-Mortes - ex Mona Lisa Royal Hôtel   \n",
       "\n",
       "                                                 url Rating  \\\n",
       "0  https://www.booking.com/hotel/fr/canal-aigues-...    8,6   \n",
       "1  https://www.booking.com/hotel/fr/la-villa-maza...    9,1   \n",
       "2  https://www.booking.com/hotel/fr/des-croisades...    8,9   \n",
       "3  https://www.booking.com/hotel/fr/saint-louis-a...    8,4   \n",
       "4  https://www.booking.com/hotel/fr/le-royal-hote...    6,9   \n",
       "\n",
       "                                         description  \\\n",
       "0  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "1  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "2  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "3  [Vous pouvez bénéficier d'une réduction Genius...   \n",
       "4  [Featuring free WiFi and a seasonal outdoor sw...   \n",
       "\n",
       "                                                bbox    Destination  \\\n",
       "0  4.16998185644106,43.5550221035115,4.2196675347...  Aigues Mortes   \n",
       "1  4.16691241748164,43.5470212030719,4.2165914962...  Aigues Mortes   \n",
       "2  4.1634570066567,43.5510319834269,4.21313939334...  Aigues Mortes   \n",
       "3  4.16450314127171,43.549254254998,4.21418406165...  Aigues Mortes   \n",
       "4  4.17297379558578,43.5584304713006,4.2226622861...  Aigues Mortes   \n",
       "\n",
       "         Lat       Lon  \n",
       "0  43.572988  4.194825  \n",
       "1  43.564987  4.191752  \n",
       "2  43.568997  4.188298  \n",
       "3  43.567220  4.189344  \n",
       "4  43.576396  4.197818  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbbox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "\n",
    "dfbbox.to_csv(\"Hotels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8af8a5d58c03aea7fed271afda0c18009a73827ffd40ee9cf6d51ffb766ed25"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
